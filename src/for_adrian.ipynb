{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml._\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "originalDf = [Time: int, V1: string ... 30 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Time: int, V1: string ... 30 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//read the full file\n",
    "val originalDf = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"mode\",\"DROPMALFORMED\")\n",
    "    .load(\"../data/creditcard.csv\") // <- change\n",
    "    .withColumn(\"id\", monotonicallyIncreasingId)\n",
    "    .withColumn(\"Time\",'Time.cast(\"Int\"))\n",
    "    .withColumn(\"Class\", 'Class.cast(\"Int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "floatColumns = Array(V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val floatColumns = originalDf.columns.filter(x => (x contains \"V\") || (x contains \"Amount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "casted_df = [Time: int, V1: float ... 30 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Time: int, V1: float ... 30 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//cast all columns containing float to float\n",
    "val casted_df = floatColumns.foldLeft(originalDf){ case (acc, col) => acc.withColumn(col, originalDf(col).cast(\"Float\"))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "casted_df_head = [Time: int, V1: float ... 30 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Time: int, V1: float ... 30 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// take only one row\n",
    "val casted_df_head = casted_df.where('id === 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount, Class, id]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casted_df_head.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model = pipeline_80e25ff27998\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_80e25ff27998"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//load the model\n",
    "val model = PipelineModel.load(\"../data/logistic_regression_model\") // <- change here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions = [Time: int, V1: float ... 36 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Time: int, V1: float ... 36 more fields]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// the model needs a dataframe to make a prediction\n",
    "// so you will need to make a dataframe made of one row, as I did above\n",
    "// Sorry about that...\n",
    "val predictions = model.transform(casted_df_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- V1: float (nullable = true)\n",
      " |-- V2: float (nullable = true)\n",
      " |-- V3: float (nullable = true)\n",
      " |-- V4: float (nullable = true)\n",
      " |-- V5: float (nullable = true)\n",
      " |-- V6: float (nullable = true)\n",
      " |-- V7: float (nullable = true)\n",
      " |-- V8: float (nullable = true)\n",
      " |-- V9: float (nullable = true)\n",
      " |-- V10: float (nullable = true)\n",
      " |-- V11: float (nullable = true)\n",
      " |-- V12: float (nullable = true)\n",
      " |-- V13: float (nullable = true)\n",
      " |-- V14: float (nullable = true)\n",
      " |-- V15: float (nullable = true)\n",
      " |-- V16: float (nullable = true)\n",
      " |-- V17: float (nullable = true)\n",
      " |-- V18: float (nullable = true)\n",
      " |-- V19: float (nullable = true)\n",
      " |-- V20: float (nullable = true)\n",
      " |-- V21: float (nullable = true)\n",
      " |-- V22: float (nullable = true)\n",
      " |-- V23: float (nullable = true)\n",
      " |-- V24: float (nullable = true)\n",
      " |-- V25: float (nullable = true)\n",
      " |-- V26: float (nullable = true)\n",
      " |-- V27: float (nullable = true)\n",
      " |-- V28: float (nullable = true)\n",
      " |-- Amount: float (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      " |-- Amount_vector: vector (nullable = true)\n",
      " |-- Amount_scaled: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-----+----------+--------------------+                         \n",
      "| id|Time|Amount|Class|prediction|         probability|\n",
      "+---+----+------+-----+----------+--------------------+\n",
      "|  1|   0|  2.69|    0|       0.0|[0.99982720734412...|\n",
      "+---+----+------+-----+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// \"prediction\" is the predicted class\n",
    "// \"Class\" is the ground truth (the original class)\n",
    "// \"probability\" (no need to look at it) is an array of 2 values, the first value is the probability the value\n",
    "// is of class 0, the second is the probability the value is of class 1\n",
    "// in this case the model is sure the record is legit, because I used the same record in the training process, so \n",
    "// he \"kind of remembers it\" (not true strictlyb speaking)\n",
    "predictions.select('id,'Time,'Amount,'Class,'prediction, 'probability).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 74:>                                                         (0 + 4) / 4]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "asd = Array(3)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "val asd = predictions.select('Class,'prediction).collect.map(\n",
    "    row => {\n",
    "        val pred = row.getDouble(1).toInt\n",
    "        val theClass = row.getInt(0)\n",
    "        s\"$pred$theClass\" match {\n",
    "            case \"11\" => 0 //tp\n",
    "            case \"01\" => 1 //fn\n",
    "            case \"10\" => 2 //fp\n",
    "            case \"00\" => 3 //tn\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
